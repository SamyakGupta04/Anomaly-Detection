{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/walmart/drive', exist_ok=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/walmart/drive')\n"
      ],
      "metadata": {
        "id": "nOw5RKgc68zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be23sryn6CQP",
        "outputId": "acd4ca87-ba04-4de9-92f4-d53877e55b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This process may take 5 to 10 minutes, depending on the performance of your computer.\n",
            "\n",
            "\n",
            "\n",
            "The pre-processing phase of the  Monday-WorkingHours.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Tuesday-WorkingHours.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Wednesday-workingHours.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Friday-WorkingHours-Morning.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX  file is completed.\n",
            "\n",
            "The pre-processing phase of the  Friday-WorkingHours-Afternoon-DDos.pcap_ISCX  file is completed.\n",
            "\n",
            "mission accomplished!\n",
            "Total operation time: =  434.0625455379486 seconds\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "seconds = time.time()\n",
        "%matplotlib inline\n",
        "\n",
        "number = \"0123456789\"\n",
        "\n",
        "# CSV files names\n",
        "csv_files = [\n",
        "    \"Monday-WorkingHours.pcap_ISCX\",\n",
        "    \"Tuesday-WorkingHours.pcap_ISCX\",\n",
        "    \"Wednesday-workingHours.pcap_ISCX\",\n",
        "    \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX\",\n",
        "    \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX\",\n",
        "    \"Friday-WorkingHours-Morning.pcap_ISCX\",\n",
        "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX\",\n",
        "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX\",\n",
        "]\n",
        "\n",
        "# Headers of columns\n",
        "main_labels = [\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Protocol\",\"Timestamp\",\"Flow Duration\",\"Total Fwd Packets\",\n",
        "   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
        "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
        "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
        "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
        "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
        "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
        "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"faulty-Fwd Header Length\",\"Fwd Avg Bytes/Bulk\",\n",
        "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
        "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
        "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",\"Label\",\"External IP\"]\n",
        "\n",
        "main_labels2 = main_labels.copy()\n",
        "header_str = \",\".join(main_labels) + \"\\n\"\n",
        "flag = True\n",
        "\n",
        "base_path = \"/walmart/drive/My Drive/walmart/CSVs/\"\n",
        "output_path = \"/walmart/drive/My Drive/walmart/all_data.csv\"\n",
        "\n",
        "\n",
        "# Remove existing merged file if present\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "for i in range(len(csv_files)):\n",
        "    tmp_file_path = os.path.join(\"/content\", f\"{i}.csv\")\n",
        "\n",
        "    # Write header to temp file\n",
        "    with open(tmp_file_path, \"w\") as ths:\n",
        "        ths.write(header_str)\n",
        "\n",
        "        input_file = os.path.join(base_path, csv_files[i] + \".csv\")\n",
        "        with open(input_file, \"r\", encoding='utf-8', errors='ignore') as file:\n",
        "            while True:\n",
        "                line = file.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                try:\n",
        "                    if line[0] in number:\n",
        "                        line = line.replace(\" â€“ \", \" - \")\n",
        "                        line = line.replace(\"inf\", \"0\")\n",
        "                        line = line.replace(\"Infinity\", \"0\")\n",
        "                        line = line.replace(\"NaN\", \"0\")\n",
        "                        ths.write(line)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    # Read cleaned file\n",
        "    df = pd.read_csv(tmp_file_path, low_memory=False)\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    string_features = [\"Flow Bytes/s\", \"Flow Packets/s\"]\n",
        "    for col in string_features:\n",
        "        df[col] = df[col].replace('Infinity', -1)\n",
        "        df[col] = df[col].replace('NaN', 0)\n",
        "        numeric_values = []\n",
        "        for val in df[col]:\n",
        "            try:\n",
        "                numeric_values.append(int(float(val)))\n",
        "            except:\n",
        "                numeric_values.append(val)\n",
        "        df[col] = numeric_values\n",
        "\n",
        "    # Detect string (categorical) columns\n",
        "    str_feats = []\n",
        "    for col in main_labels2:\n",
        "        if df[col].dtype == \"object\":\n",
        "            str_feats.append(col)\n",
        "\n",
        "    try:\n",
        "        str_feats.remove(\"Label\")\n",
        "    except:\n",
        "        print(\"Warning: 'Label' column not found in string_features list\")\n",
        "\n",
        "    labelencoder_X = preprocessing.LabelEncoder()\n",
        "    for col in str_feats:\n",
        "        try:\n",
        "            df[col] = labelencoder_X.fit_transform(df[col])\n",
        "        except:\n",
        "            df[col] = df[col].replace(\"Infinity\", -1)\n",
        "\n",
        "    # Drop faulty column\n",
        "    df = df.drop(main_labels2[61], axis=1)\n",
        "\n",
        "    # Merge to all_data.csv\n",
        "    if flag:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        flag = False\n",
        "    else:\n",
        "        df.to_csv(output_path, index=False, header=False, mode=\"a\")\n",
        "\n",
        "    # Remove temp file\n",
        "    os.remove(tmp_file_path)\n",
        "    print(f\"The pre-processing phase of the {csv_files[i]} file is completed.\\n\")\n",
        "\n",
        "print(\"Mission accomplished!\")\n",
        "print(\"Total operation time: = \", time.time() - seconds, \"seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sDsiQ1x6CQS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}